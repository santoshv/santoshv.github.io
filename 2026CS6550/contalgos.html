<!DOCTYPE html>
<link rel="stylesheet" href="style.css">
<html>
<body>
<div>
<h2>CS6550: Continuous Algorithms: Optimization and Sampling</h2>
<h4>Spring 2026. MW: 2-3:15pm, Klaus 1447</h4>
<h4>Santosh Vempala, Klaus 2222, Office hour: TBD.<br>
TAs: TBD.</h4>

<p>The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a 
continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have 
desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is 
derived from this by appropriate discretization. We will use this viewpoint to develop several general techniques and build up to the state-of-the-art in polynomial algorithms for optimization and sampling. These techniques include:
<ul>
<li>Reduction</li>
<li>Elimination</li>
<li>Conditioning</li>
<li>Geometrization</li>
<li>Expansion</li>
<li>Sparsification</li>
<li>Acceleration</li>
<li>Decomposition</li>
<li>Discretization</li>
</ul>


<p><b>Prerequisite</b>: CS6515 (graduate algorithms) or equivalent, basic knowledge of probability and linear algebra.</p><br>

<b><a href="https://github.com/YinTat/optimizationbook/blob/main/main.pdf">Textbook</a></b> (draft, with Yin Tat Lee)<br></p><br>

<p><b>Grading</b>:<br>
HW (TBD%): Biweekly HW. You are encouraged to collaborate on HW, but must write your own solutions. Submit via gradescope (link on canvas).<br>
Two in-class mid-term exams (TBD% each). <br> 
Send comments on textbook via this <a href="https://docs.google.com/forms/d/e/1FAIpQLScN4UUO8Oy2JZTg9oJbEnQb0WNQtUCJiZVqgTs4eOoiCaVwjw/viewform?vc=0&c=0&w=1&flr=0">form</a> (up to 0.5% per chapter and 5% total).<br>
Bonus (up to 10%): simpler proofs, new exercises.<br>
</p>
<br>




<b>Schedule</b> (tentative):
<ol>
<li> Introduction<br>
Jan 12. Course overview. Ch 1: Convexity. <a href="https://www.dropbox.com/scl/fi/i1q15eqqb0ekkhl8smstw/hw1.pdf?rlkey=56w0z2zaw0r0rlntmjwtrda53&e=1&st=ys9dp5q1&dl=0">HW1</a> (due Jan 15 by 5pm).<br> 
Jan 14. Ch 2: Gradient descent. <br>
Jan 19. MLK Holiday<br>
Jan 21. Ch 2, Ch 8: Strong convexity; Gradient flow; Langevin dynamics.<br>
Jan 26,28. Ch 8: Sampling and Diffusion. <br>

<li>Elimination<br>

Feb 2. Ch 3: Cutting Plane Method: Ellipsoid Algorithm. <br>
Feb 4. Ch 3: Cutting Plane Method: Center of Gravity. <br>

<li>Reduction<br>

Feb 9. Ch 9: CPM: Computing the Volume. <br>
Feb 11. Ch 4: Oracles and Duality. <br>
Feb 18. Ch 4: Optimization and Separation from Membership. <br>

<li>Geometrization I: Euclidean<br>

Feb 25, 27. Sampling, Markov chains and Conductance. <br>
Mar 4. Polytime Sampling with the Ball Walk. <br>
Mar 6. Euclidean Isoperimetry and the Localization Lemma. <br>

<li>Geometrization II: Non-Euclidean<br>

Mar 11. Hit-and-Run: Rapid Mixing from any start. <br>
Mar 13. Midterm I.<br>
Mar 18, 20. The Central Path Method. <br>
Mar 23, 25. Spring break <br>
Mar 30. The Newton Method. <br>

<li>Acceleration<br>
Apr 1. The Interior-Point Method. <br>
Apr 6. Midterm II. <br> 
Apr 8. Simulated Annealing and Gaussian Cooling. <br>
Apr 13. Integration and Rounding. <br> 
Apr 15. (Riemannian) Hamiltonian Monte Carlo. <br>

<li>Discretization<br>
Apr 20, 22. Lattices and Basis Reduction.  <br>
Apr 27. Student's choice! <br> 
</ol>


</p>
</div>
</body>



