<!DOCTYPE html>
<link rel="stylesheet" href="style.css">
<html>
<body>
<div>
<h2>CS4540/CS8803: (Is there) A(n Algorithmic) Theory of (Artificial) Intelligence(?)</h2>
<h4>Fall 2023. MW: 9:30-10:45, Mason 5134</h4>
<h4>Santosh Vempala, Klaus 2222, Office hour: Mon 1:30-3pm (or by appt) <br>
TAs: Xinyuan Cao OH: Tue 2-4pm, Klaus 3100. Mirabel Reid OH: Wed 12-2pm, Klaus 2100 </h4>

<p>
What is the basis of intelligence? How can neural networks, artificial or natural, provably lead to perception, learning and cognition? We'll begin by studying a few classics, highlight the gaps and challenges posed by modern artificial models and the brain, and then discuss more recent rigorous proposals. Note that this is a mathematically rigorous course and the focus is on what is provably true related to this topic. Students interested in the latest developments in empirical AI should take other courses.

<p><b>Prerequisites</b>: algorithms, linear algebra.</p>

<p><b>Grading</b>:<br>
HW (40%): 4-6 Problem sets.<br>
Exams (40%): Two in-class exams. No final. No exams for 8803 students. <br>
Project (20%): Each team can choose one of the following options: (a) read a paper and provide an insightful explanation/simpler proof/ generalization (b) prove a conjecture (c) formulate a precise conjecture supported by experimental or theoretical evidence. A list of candidate projects will be provided.<br>
Candidate Project topics are available  <a href="Project Guidelines.pdf">here</a>. 
</p>

<b>Readings</b> (under construction!)
    <ol>
        <li><a href="https://link.springer.com/article/10.1007/BF02478259">The McCulloch-Pitts Neuron</a></li>
        <li><a href="https://dl.acm.org/doi/pdf/10.1145/1968.1972">PAC Learning</a></li>
        <li><a href="https://link.springer.com/article/10.1007/BF00116827">Winnow and Weighted Majority</a></li>
        <li><a href="https://dl.acm.org/doi/pdf/10.1145/76359.76371">VC dimension</a></li>
        <li><a href="https://link.springer.com/article/10.1007/bf00994018">Support Vector Machines</a></li>
        <li><a href="https://link.springer.com/article/10.1007/s10994-006-6265-7">Random Projection for Learning</a></li>
        <li><a href="https://www.sciencedirect.com/science/article/pii/S002200009791504X">Boosting</a></li>
        <li><a href="https://static.renyi.hu/~p_erdos/1960-10.pdf">Random Graphs</a></li>
        <li><a href="https://www.combinatorics.org/ojs/index.php/eljc/article/download/v6i1r17/pdf">The Regularity Lemma</a></li>
        <li>Dynamics and Equilibria: the Brouwer-Kakutani Fixed Point Theorems</li>
	<li>Learning Finite Automata</li>
        <li>Cell Assemblies: Hebb, D. O. <em>The organization of behavior: A neuropsychological theory.</em> Psychology press.</li>
        <li><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2001893117">The Assembly Hypothesis</a></li>
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/10ce03a1ed01077e3e289f3e53c72813-Paper.pdf">Generalization in Deep Learning</a></li>
        <li><a href="https://openreview.net/pdf?id=BCBac5kkg5G">Recurrent Neural Networks</a></li>
        <li>LLMs??!</li>
    </ol>



<b>Schedule</b>
<ul>
<li>
Aug 21, 23. Course overview. The Perceptron Algorithm <a href="Lec1_0821_0823.pdf">Notes</a>. 

<li>
Aug 28, 30. Weighted Majority and Winnow <a href="Lec2_0828_0830.pdf">Notes</a>. 

<li>
Sep 6, 11, 13. PAC learning, Sample Complexity, VC dimension <a href="Lec3_0906_0911_0913.pdf">Notes</a>. 

<li>
Sep 18, 20. Support Vector Machines <a href="Lec4_0918_svm.pdf">Notes</a>. Random Projection for Learning <a href="Lec5_0920.pdf">Notes</a>.

<li>
Sep 25, 27. Boosting, Concentration Inequalities <a href="Lec6_0925.pdf">Notes</a>. 

<li>
Oct 2, 11. Random Graphs <a href="Lec7_0925.pdf">Notes</a>, <a href="Lec8_1011.pdf">Notes</a>.

<li>
Oct 4. Midterm I. 

<li> 
Oct 16, 18. Fixed Point Iterations and Nash Equilibrium <a href="Lec9_1016new.pdf">Notes</a>. 

<li>
Oct 23. Regularity Lemmas <a href="Lec10_1023.pdf">Notes</a>.  

<li>
Oct 25, 30. Assemblies of Neurons. <a href="https://github.com/santoshv/santoshv.github.io/blob/master/2023CS4540/NEMO.pdf">slides</a>, simulations: <a href="https://github.com/mdabagia/learning-with-assemblies">small</a> and <a href="https://github.com/dmitropolsky/assemblies">large</a>.

<li> 
Nov 1. Angluin's algorithm for learning a Finite Automaton <a href="Lec11_1101.pdf">Notes</a>. 

<li>
Nov 6, 8. Neural Networks: Architecture, Optimization, Generalization <a href="Lec12_1106.pdf">Notes</a>.

<li>
Nov 13. Rademacher Complexity  <a href="Lec13_1113.pdf">Notes</a>. 

<li>
Nov 15. Midterm II

<li>
Nov 20. The Complexity of Human Cognitive Processing. 

<li>
Nov 27, 29. Language

<li>
Dec 4. Language models and Next-word Prediction

</ul>



</div>
</body>
